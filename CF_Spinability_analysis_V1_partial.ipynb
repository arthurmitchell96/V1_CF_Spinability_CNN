{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7a163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyabf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import data from spreadsheet \n",
    "#This has many columns with redundant data for this project and was made for people to read in excel\n",
    "ABF_data = pd.read_excel(*Path to file here*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.eq(\"R347P\").any(axis=5)]\n",
    "#Pull data only with the R347P mutation\n",
    "R347P_data = ABF_data.loc[ABF_data['Mutation']=='R347P']\n",
    "del(ABF_data)\n",
    "#Pull ER data, ER data referes to spinability recording\n",
    "R347P_data = R347P_data.loc[R347P_data['Type'] =='ER']\n",
    "#And pull data where DMSO is applied to the culture\n",
    "R347P_data_dmso=R347P_data.loc[R347P_data[\"Drugs\"]==\"DMSO\"]\n",
    "#Pull data where '3VX' is applied\n",
    "R347P_data_vx=R347P_data.loc[R347P_data[\"Drugs\"]==\"VX445, VX661, VX770\"]\n",
    "\n",
    "#Create the train/test split for DMSO data\n",
    "R347P_DMSO_train, R347P_DMSO_test = train_test_split(R347P_data_dmso, test_size=0.2)\n",
    "#Create the train/test split for the 3VX data\n",
    "R347P_vx_train, R347P_vx_test = train_test_split(R347P_data_vx, test_size=0.2)\n",
    "#free up memory\n",
    "del(R347P_data)\n",
    "#create the train dataset\n",
    "train = pd.concat([R347P_DMSO_train, R347P_vx_train])\n",
    "\n",
    "#Create the test dataset\n",
    "test = pd.concat([R347P_DMSO_test, R347P_vx_test])\n",
    "\n",
    "#Resetting indext to allow for innmumeration later\n",
    "train =  train.reset_index()\n",
    "test =  test.reset_index()\n",
    "\n",
    "#Free up memory \n",
    "del(R347P_DMSO_train)\n",
    "del(R347P_DMSO_test)\n",
    "del(R347P_vx_train)\n",
    "del(R347P_vx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This section populates the train array\n",
    "\n",
    "#Initalise array to write ABF files to \n",
    "ABF_train_as_array = []\n",
    "#File locations\n",
    "abf_loc = *Location Here*\n",
    "\n",
    "\n",
    "#for loop to cycle through train array and extract ABF file names\n",
    "for index, row in train.iterrows():\n",
    "    #Loading in the ABD files\n",
    "    temp_array = pyabf.ABF(abf_loc+str(row['Recording name']) + \".abf\")\n",
    "    #Specify which channel to call\n",
    "    temp_array.setSweep(sweepNumber=0, channel=0)\n",
    "    #ABF files load y axis (the current) and x axis (time for each array poin), X is not relevant in this case as it can be claulated via the y axis\n",
    "    temp_y = temp_array.sweepY\n",
    "    \n",
    "    #The first 2-10 seconds are not useful in this data. Withdraw time is the time the pipette withdrawal starts\n",
    "    withdraw_start = row['Withdraw Time']\n",
    "    #Time per array element is given here from the ABF data\n",
    "    time_per_array = temp_array.sweepX[1] \n",
    "    #The point in the array where the withdawal startes is calulaed by the start time (seconds)/Time per array element     \n",
    "    withdraw_start_point = round(withdraw_start/time_per_array)\n",
    "    #Normalise the data as the current can vary (with pipette resistant and bath electrode), this also allow normaliseation for the shape of the curve\n",
    "    norm_y = (temp_y-np.min(temp_y))/(temp_y[withdraw_start_point]-np.min(temp_y))\n",
    "    #Trim useless data from array\n",
    "    norm_y = norm_y[withdraw_start_point:]\n",
    "    # downsampling all arrays so they are the same size on the time axis allowing for analysis of the shape\n",
    "    #This is also done as there is a large variation in the time of the recordings which makes using a CNN more complicated\n",
    "    #shortest array is length 36320\n",
    "    window_size = round(len(norm_y)/36320)\n",
    "    #Find the remainder and add on as paddding, to allow for uniform downsampling resulting in uniform array sizes\n",
    "    padding = len(norm_y)%window_size\n",
    "    norm_y.resize((len(norm_y+padding)))\n",
    "    #array to assign re-sized arrays to\n",
    "    temp_norm_y = []\n",
    "    #end point is to allow for the last pass of the median window\n",
    "    end = int(len(norm_y)-window_size)\n",
    "    #shift the window the length of the window each time\n",
    "    for i in range (0,end,window_size+1):\n",
    "        #caclulte end of median window\n",
    "        window_end = i+window_size\n",
    "        #median filter is used as traces can suffer from noise, the median filter will reduce noise and array size\n",
    "        median_val = np.median(norm_y[i:window_end])\n",
    "        #append the median to the final array\n",
    "        temp_norm_y.append(median_val)\n",
    "    #convert to array\n",
    "    temp_norm_y = np.asarray(temp_norm_y) \n",
    "    #the resizing does not work perfectly! I think this is due to rounding and calculation of the window size. \n",
    "    #This needs fixing but the results are close, so for not cropping, or zero padding. This needs fixing but is not urgent\n",
    "    temp_norm_y.resize(36320)\n",
    "    #convert to array\n",
    "    temp_array_np = np.array(temp_norm_y)\n",
    "    #add to final data array\n",
    "    ABF_train_as_array.append(temp_norm_y)\n",
    "    \n",
    "\n",
    "#print(ABF_train_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de177955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initalise array to write ABF files to \n",
    "ABF_test_as_array = []\n",
    "#File locations\n",
    "abf_loc = 'SICM_files/'\n",
    "\n",
    "\n",
    "#for loop to cycle through train array and extract ABF file names\n",
    "for index, row in test.iterrows():\n",
    "    #Loading in the ABD files\n",
    "    temp_array = pyabf.ABF(abf_loc+str(row['Recording name']) + \".abf\")\n",
    "    #Specify which channel to call\n",
    "    temp_array.setSweep(sweepNumber=0, channel=0)\n",
    "    #ABF files load y axis (the current) and x axis (time for each array poin), X is not relevant in this case as it can be claulated via the y axis\n",
    "    temp_y = temp_array.sweepY\n",
    "    \n",
    "    #The first 2-10 seconds are not useful in this data. Withdraw time is the time the pipette withdrawal starts\n",
    "    withdraw_start = row['Withdraw Time']\n",
    "    #Time per array element is given here from the ABF data\n",
    "    time_per_array = temp_array.sweepX[1] \n",
    "    #The point in the array where the withdawal startes is calulaed by the start time (seconds)/Time per array element     \n",
    "    withdraw_start_point = round(withdraw_start/time_per_array)\n",
    "    #Normalise the data as the current can vary (with pipette resistant and bath electrode), this also allow normaliseation for the shape of the curve\n",
    "    norm_y = (temp_y-np.min(temp_y))/(temp_y[withdraw_start_point]-np.min(temp_y))\n",
    "    #Trim useless data from array\n",
    "    norm_y = norm_y[withdraw_start_point:]\n",
    "    # downsampling all arrays so they are the same size on the time axis allowing for analysis of the shape\n",
    "    #This is also done as there is a large variation in the time of the recordings which makes using a CNN more complicated\n",
    "    #shortest array is length 36320\n",
    "    window_size = round(len(norm_y)/36320)\n",
    "    #Find the remainder and add on as paddding, to allow for uniform downsampling resulting in uniform array sizes\n",
    "    padding = len(norm_y)%window_size\n",
    "    norm_y.resize((len(norm_y+padding)))\n",
    "    #array to assign re-sized arrays to\n",
    "    temp_norm_y = []\n",
    "    #end point is to allow for the last pass of the median window\n",
    "    end = int(len(norm_y)-window_size)\n",
    "    #shift the window the length of the window each time\n",
    "    for i in range (0,end,window_size+1):\n",
    "        #caclulte end of median window\n",
    "        window_end = i+window_size\n",
    "        #median filter is used as traces can suffer from noise, the median filter will reduce noise and array size\n",
    "        median_val = np.median(norm_y[i:window_end])\n",
    "        #append the median to the final array\n",
    "        temp_norm_y.append(median_val)\n",
    "    #convert to array\n",
    "    temp_norm_y = np.asarray(temp_norm_y) \n",
    "    #the resizing does not work perfectly! I think this is due to rounding and calculation of the window size. \n",
    "    #This needs fixing but the results are close, so for not cropping, or zero padding. This needs fixing but is not urgent\n",
    "    temp_norm_y.resize(36320)\n",
    "    #convert to array\n",
    "    temp_array_np = np.array(temp_norm_y)\n",
    "    #add to final data array\n",
    "    ABF_test_as_array.append(temp_norm_y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7661d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of the script writes the data to CSV files \n",
    "\n",
    "import csv\n",
    "\n",
    "with open('train_data.csv', 'w') as f:\n",
    "    \n",
    "    # Create a CSV writer object that will write to the file 'f'\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    # Write the field names (column headers) to the first row of the CSV file\n",
    "    #csv_writer.writerow(fields)\n",
    "    \n",
    "    # Write all of the rows of data to the CSV file\n",
    "    csv_writer.writerows(ABF_train_as_array)\n",
    "# close the file\n",
    "f.close()\n",
    "\n",
    "\n",
    "# open the file in the write mode\n",
    "with open('train_labels.csv', 'w') as f:\n",
    "\n",
    "    # create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write a row to the csv file\n",
    "    writer.writerows(train['Drugs'])\n",
    "\n",
    "# close the file\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dtype for data\n",
    "train_ABF_as_np_array = np.asarray(ABF_train_as_array,dtype=object)\n",
    "test_ABF_as_np_array = np.asarray(ABF_test_as_array,dtype=object)\n",
    "#find minimum value, This is now earlier in the script\n",
    "#min_l = inf\n",
    "#find smllest array length in the training data\n",
    "#for l in train_ABF_as_np_array:\n",
    " #   Length = len(l)\n",
    "\n",
    "  #  if Length < max_l:\n",
    "   #     min_l = Length\n",
    "\n",
    "\n",
    "\n",
    "#Length = inf\n",
    "\n",
    "#find smllest array length in the test data\n",
    "#for l in test_ABF_as_np_array:\n",
    " #   Length = len(l)\n",
    "    #print(Length)\n",
    "  #  if Length < max_l:\n",
    "   #     min_l = Length\n",
    "#print('Max length')\n",
    "#print(max_l)\n",
    "#cycle \n",
    "#count =0\n",
    "#for l in train_ABF_as_np_array:\n",
    "\n",
    " #   temp = np.array(l)\n",
    "\n",
    "  #  train_ABF_as_np_array[count] = temp\n",
    "\n",
    "   # count = count + 1\n",
    " \n",
    " #count = 0    \n",
    "#for l in test_ABF_as_np_array:\n",
    "\n",
    " #   Length = max_l - len(l)\n",
    "\n",
    "  #  temp = np.array(l)\n",
    "\n",
    "   # temp.resize(max_l)\n",
    "\n",
    "    #test_ABF_as_np_array[count] = temp\n",
    "\n",
    "    #count = count + 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For debugging to check arrays re the same length\n",
    "#for i in train_ABF_as_np_array:\n",
    " #   print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41132c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####This part initlises and trains the model ####\n",
    "##inital CNN\n",
    "input_shape= (36320,1)\n",
    "#populates the label array \n",
    "labels_binary = []\n",
    "#Cycles through \n",
    "for i in train['Drugs']:\n",
    "    if i == 'DMSO':\n",
    "        labels_binary.append(0)\n",
    "    else:\n",
    "        labels_binary.append(1)\n",
    "#convert data type        \n",
    "labels = np.asarray(labels_binary).astype(np.int32)\n",
    "\n",
    "\n",
    "train_data = np.asarray(train_ABF_as_np_array).astype(np.int32)\n",
    "test_data = np.asarray(test_ABF_as_np_array).astype(np.int32)\n",
    "\n",
    "#populate test labels\n",
    "labels_binary_test = []\n",
    "for i in test['Drugs']:\n",
    "    if i == 'DMSO':\n",
    "        labels_binary_test.append(0)\n",
    "    else:\n",
    "        labels_binary_test.append(1)\n",
    "     \n",
    "\n",
    "#other inputs for the model \n",
    "#ASL dpeth\n",
    "train_depths = np.asarray(train['ASL Depth (mm)']).astype(np.int32)\n",
    "#infection\n",
    "train_infection = np.asarray(train['Infected (0 no 1 day before infection 2 infected while measuring)']).astype(np.int32)\n",
    "#snap length\n",
    "train_snap = np.asarray(train['Snap Lengths (um)']).astype(np.int32)\n",
    "#90% current reduction\n",
    "train_90_current = np.asarray(train['90% current redcution point']).astype(np.int32)\n",
    "\n",
    "\n",
    "#test inputs\n",
    "test_depths = np.asarray(test['ASL Depth (mm)']).astype(np.int32)\n",
    "test_infection = np.asarray(test['Infected (0 no 1 day before infection 2 infected while measuring)']).astype(np.int32)\n",
    "test_snap = np.asarray(test['Snap Lengths (um)']).astype(np.int32)\n",
    "test_90_current = np.asarray(test['90% current redcution point']).astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#transpose arrays to match indicies\n",
    "input_train = np.asarray([train_depths,train_infection,train_snap,train_90_current]).T\n",
    "\n",
    "input_test = np.asarray([test_depths,test_infection,test_snap,test_90_current]).T\n",
    "\n",
    "labels_test = np.asarray(labels_binary_test).astype(np.int32)\n",
    "#val_dataset = [test_data, input_test]\n",
    "#val_dataset = np.asarray(val_dataset).astype(np.int32)\n",
    "#val_dataset = np.asarray(val_dataset).astype(np.int32)\n",
    "#initilise column 1, this is about as big as my laptop gpu can handel\n",
    "input_A = tf.keras.layers.Input(input_shape)\n",
    "x = tf.keras.layers.Conv1D(32, kernel_size=1028, activation='relu')(input_A)\n",
    "#x = tf.keras.layers.Dropout(0.2)(x)\n",
    "#x = tf.keras.layers.Conv1D(32, kernel_size=512, activation='relu')(x)\n",
    "#x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Conv1D(32, kernel_size=256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Conv1D(32, kernel_size=128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "#x = tf.keras.layers.Conv1D(32, kernel_size=64, activation='relu')(x)\n",
    "#x = tf.keras.layers.Dropout(0.2)(x)\n",
    "#x = tf.keras.layers.Conv1D(32, kernel_size=32, activation='relu')(x)\n",
    "#x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Conv1D(32, kernel_size=16, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "#x = tf.keras.layers.Conv1D(32, kernel_size=8, activation='relu')(x)\n",
    "#x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Conv1D(16, kernel_size=8, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Conv1D(8, kernel_size=8, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "x = tf.keras.Model(inputs=input_A, outputs=x)\n",
    "\n",
    "\n",
    "#outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "#initilise column 2\n",
    "\n",
    "input_B = tf.keras.layers.Input((4))\n",
    "x1 = tf.keras.layers.Dense(32, activation='relu')(input_B)\n",
    "x1 = tf.keras.layers.Dropout(0.2)(x1)\n",
    "x1 = tf.keras.layers.Dense(16, activation='relu')(x1)\n",
    "x1 = tf.keras.layers.Dropout(0.2)(x1)\n",
    "x1 = tf.keras.layers.Dense(8, activation='relu')(x1)\n",
    "x1 = tf.keras.layers.Flatten()(x1)\n",
    "x1 = tf.keras.Model(inputs=input_B, outputs=x1)\n",
    "\n",
    "#combine dense and CNN\n",
    "combined = tf.keras.layers.concatenate([x.output, x1.output])\n",
    "\n",
    "z = tf.keras.layers.Dense(2, activation=\"relu\")(combined)\n",
    "z = tf.keras.layers.Dense(16, activation=\"relu\")(z)\n",
    "z = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "\n",
    "#create model\n",
    "model = tf.keras.Model([input_A, input_B], z)\n",
    "#train using binary cross entropy\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#fit model\n",
    "model.fit(x=[train_data, input_train], y=labels, batch_size=4, epochs=100,validation_data=([test_data, input_test],labels_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566efed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
